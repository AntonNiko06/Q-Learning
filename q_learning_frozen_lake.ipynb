{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c7dc51d",
   "metadata": {},
   "source": [
    "# Q-Learning\n",
    "_By Anton Nikolaychuk_\n",
    "\n",
    "In this project I will be implementing grid world and frozen lake - both basic reinforcement learning environments - and train an agent to navigate them using Q-learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64f5828",
   "metadata": {},
   "source": [
    "We will start by importing our required libraries. Numpy is used mainly for random number generation while pygame is used to display our maze and training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6396ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.13.3)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pygame\n",
    "\n",
    "# Prevent showing Q-values in scientific format, for better readability\n",
    "np.set_printoptions(suppress=True, precision=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d0ed09",
   "metadata": {},
   "source": [
    "## Q-Learning Implementation\n",
    "In the following cell, we implement two seperate classes, one for the maze and one for the agent. The maze class stores all information relevant to the layout of the maze, such as column/row count, while the agent class implements the required functions for learning to navigate the maze, such as the Q-function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14369bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QMaze():\n",
    "    def __init__(self, maze_base, goal_points, small_points, step_cost):\n",
    "        \"\"\"Initialize all future required variables as well as the maze and Q-table\"\"\"\n",
    "\n",
    "        self.maze_base = maze_base\n",
    "\n",
    "        self.step_cost = step_cost\n",
    "        self.field_mapping = {\"S\": self.step_cost, \"W\": 0, \"G\": goal_points, \"T\": -goal_points, \"o\": self.step_cost, \"+\": small_points, \"-\": -small_points}\n",
    "        self.action_mapping = {0: \"left\", 1: \"down\", 2: \"right\", 3: \"up\"}\n",
    "\n",
    "        self.maze = []\n",
    "        self.column_count = None\n",
    "        self.row_count = None\n",
    "        self.start_field = None\n",
    "\n",
    "        self.construct_maze()\n",
    "\n",
    "    def construct_maze(self):\n",
    "        \"\"\"Get the maze fields as a list of characters. Set some variables containing information about the maze layout\"\"\"\n",
    "\n",
    "        rows = self.maze_base.split(\"\\n\")\n",
    "        fields_str = \"\".join(rows)\n",
    "\n",
    "        for i in range(len(fields_str)):\n",
    "            self.maze.append(fields_str[i])\n",
    "\n",
    "            if fields_str[i] == \"S\": \n",
    "                self.start_field = i\n",
    "\n",
    "        self.column_count = len(rows[0])\n",
    "        self.row_count = len(rows)\n",
    "\n",
    "\n",
    "class QAgent():\n",
    "    def __init__(self, maze, epsilon, slip_chance, learning_rate, discount_rate):\n",
    "        self.maze = maze\n",
    "        self.used_boni = []\n",
    "\n",
    "        self.epsilon = epsilon\n",
    "        self.slip_chance = slip_chance\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_rate = discount_rate\n",
    "\n",
    "        self.cur_state = self.maze.start_field\n",
    "\n",
    "        self.q_table = None\n",
    "        self.construct_q_table(method=\"zeros\")\n",
    "    \n",
    "    def construct_q_table(self, method=\"zeros\"):\n",
    "        \"\"\"Initializes the Q-table with 4 values for each field\"\"\"\n",
    "\n",
    "        if method == \"zeros\":\n",
    "            self.q_table = np.zeros((self.maze.row_count*self.maze.column_count, 4))\n",
    "        elif method==\"random\":\n",
    "            rng = np.random.default_rng(1) # rng with seed 1 instead of random.seed to not overwrite global random seed\n",
    "            self.q_table = rng.uniform(-0.3, 0.3, (self.maze.row_count*self.maze.column_count, 4))\n",
    "        \n",
    "    def update_q_table(self):\n",
    "        \"\"\"Updates the value of the previous field the agent was on using Q-learning\"\"\"\n",
    "\n",
    "        # Choose action to perform from current field\n",
    "        intended_action, slip_action = self.choose_action()\n",
    "\n",
    "        # Get the current Q-value for that action on that field\n",
    "        old_q_val = self.q_table[self.cur_state, intended_action]\n",
    "\n",
    "        # Get the new field the agent would be in after performing the chosen action\n",
    "        new_state = self.get_new_state(slip_action)\n",
    "\n",
    "        # Get the immediate reward\n",
    "        immediate_reward = self.maze.field_mapping[self.maze.maze[new_state]]\n",
    "        if new_state in self.used_boni: # if the reward/punishment of a bonus was already collected this episode, don't give it again\n",
    "            immediate_reward = self.maze.step_cost\n",
    "\n",
    "        # Get the maximum Q-value of the actions that can be performed in the new field\n",
    "        max_future_q_val = max(self.q_table[new_state])\n",
    "\n",
    "        # Calculate the new Q-value for the chosen action for the current field based on TD\n",
    "        new_q_val = old_q_val + self.learning_rate * (immediate_reward + self.discount_rate * max_future_q_val - old_q_val)\n",
    "\n",
    "        # Update the Q-table with the value\n",
    "        self.q_table[self.cur_state, intended_action] = new_q_val\n",
    "\n",
    "        # Update the state to perform the action\n",
    "        self.update_state(new_state)\n",
    "\n",
    "    def action_possible(self, action):\n",
    "        \"\"\"Determine if a given action is possible at the current state\"\"\"\n",
    "\n",
    "        new_state = self.get_new_state(action)\n",
    "\n",
    "        # Prevent moving out of bounds of the maze grid\n",
    "        if new_state < 0 or new_state >= len(self.maze.maze):\n",
    "            return False\n",
    "        # Prevent moving from end of one row to start of next or vice versa by moving right/left\n",
    "        if (not self.cur_state == 0) and (\n",
    "            (self.cur_state % self.maze.column_count == 0 and action == 0) or # start of row\n",
    "            (self.cur_state % self.maze.column_count == self.maze.column_count - 1 and action == 2)): # end of row\n",
    "            return False\n",
    "        # Prevent moving onto walls\n",
    "        if self.maze.maze[new_state] == \"W\":\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def get_possible_actions(self) -> np.ndarray:\n",
    "        \"\"\"Get a list of possible actions\"\"\"\n",
    "\n",
    "        all_actions = np.arange(4)\n",
    "        pos_actions = []\n",
    "\n",
    "        for act in all_actions:\n",
    "            if self.action_possible(act): \n",
    "                pos_actions.append(act)\n",
    "\n",
    "        return np.array(pos_actions)\n",
    "\n",
    "    def get_new_state(self, action):\n",
    "        \"\"\"Get the state the agent will be in after performing a given action\"\"\"\n",
    "\n",
    "        if action == 0: # Move left\n",
    "            new_state = self.cur_state - 1\n",
    "        elif action == 1: # Move down\n",
    "            new_state = self.cur_state + self.maze.column_count\n",
    "        elif action == 2: # Move right\n",
    "            new_state = self.cur_state + 1\n",
    "        elif action == 3: # Move up\n",
    "            new_state = self.cur_state - self.maze.column_count\n",
    "        else:\n",
    "            raise KeyError(f\"Invalid action: {action}\")\n",
    "\n",
    "        return new_state\n",
    "\n",
    "    def update_state(self, new_state):\n",
    "        \"\"\"Update the current state based on the new state\"\"\"\n",
    "\n",
    "        # Update state\n",
    "        self.cur_state = new_state\n",
    "\n",
    "        next_maze_field = self.maze.maze[new_state]\n",
    "\n",
    "        # If the agent lands on a bonus field, keep track of it until the end of the episode in order to prevent using bonus multiple times\n",
    "        if next_maze_field == \"+\" or next_maze_field == \"-\":\n",
    "            self.used_boni.append(new_state)\n",
    "\n",
    "        # If the agent lands on a goal/trap field, reset its position to the start field and forget about the boni it collected\n",
    "        if next_maze_field == \"G\" or next_maze_field == \"T\":\n",
    "            self.cur_state = self.maze.start_field\n",
    "            self.used_boni = []\n",
    "\n",
    "    def choose_action(self):\n",
    "        \"\"\"Choose action using epsilon-greedy policy and incorporate slip chance\"\"\"\n",
    "\n",
    "        pos_actions = self.get_possible_actions()\n",
    "        intended_action = None\n",
    "        slip_action = None\n",
    "\n",
    "        if np.random.random() < 1-self.epsilon: \n",
    "            # Choose best action based on highest Q-value\n",
    "\n",
    "            cur_q_vals = self.q_table[self.cur_state] # get the Q-values for all actions of the current field\n",
    "            pos_q_vals = cur_q_vals[pos_actions] # filter out the actions that can't be performed\n",
    "\n",
    "            # Get list of actions that have the highest Q-value (as there might be multiple)\n",
    "            max_q_val = np.max(pos_q_vals)\n",
    "            best_actions = np.where(cur_q_vals == max_q_val)[0]\n",
    "            best_actions = [act for act in best_actions if act in pos_actions]\n",
    "            \n",
    "            # Randomly pick one of the best actions or the one best action (if there is just one)\n",
    "            intended_action = np.random.choice(best_actions)\n",
    "        else: \n",
    "            # Choose random action\n",
    "            intended_action = pos_actions[np.random.randint(0, len(pos_actions))]\n",
    "\n",
    "        slip_action = intended_action\n",
    "\n",
    "        # There is a random chance for the agent to \"slip\" and perform an action different from the one he intended to perform\n",
    "        if np.random.random() < self.slip_chance:\n",
    "            slip_action = pos_actions[np.random.randint(0, len(pos_actions))]\n",
    "        \n",
    "        return intended_action, slip_action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92873ba8",
   "metadata": {},
   "source": [
    "## Setup\n",
    "In the following cell, you can adjust the layout of the maze by changing the \"maze_base\" variable. The field mappings are as follows: \n",
    "- \"o\" - empty field\n",
    "- \"W\" - wall/obstacle\n",
    "- \"S\" - start field\n",
    "- \"G\" - goal field\n",
    "- \"T\" - trap/hole in ice\n",
    "- \"+\" -  small reward\n",
    "- \"-\" - small punishment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1c80ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chose the maze layout here (all rows should be the same size and all columns should be the same size, but row and column counts do not have to match. Don't add whitespaces)\n",
    "maze_base = \"\"\"\n",
    "ooo+ooo\n",
    "ooooooo\n",
    "SoTTToo\n",
    "ooooooG\n",
    "\"\"\"[1:-1]\n",
    "\n",
    "# Here you can assign the rewards/punishments for landing on certain fields\n",
    "# +-goal_points: G/T  |  +-small_points: +/-  |  step_cost: o (note that step cost should be negative)\n",
    "maze = QMaze(maze_base, goal_points=1, small_points=0.2, step_cost=-0.3)\n",
    "\n",
    "# Here you can change the hyperparameters for the training of the agent\n",
    "# epislon: Probability of the agent to perform a random action instead of the best one he knows\n",
    "# slip_chance: Probability of the agent to \"slip\" and perform a random action instead of the one he intended to perform\n",
    "#              -> If set to 0, then the environment becomes the Gridworld environment\n",
    "#              -> If set to a value between 0 and 1, then the environment becomes the frozen lake environment\n",
    "# learning_rate: Governs how much the agent should learn with each update\n",
    "# discount_rate: Governs how important future rewards are to the agent \n",
    "agent = QAgent(maze, epsilon=0.15, slip_chance=0.2, learning_rate=0.05, discount_rate=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9ef192",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c30d7d0",
   "metadata": {},
   "source": [
    "#### Pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803e9c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize pygame window\n",
    "pygame.init()\n",
    "CELL_SIZE = 80\n",
    "WINDOW_WIDTH = maze.column_count * CELL_SIZE\n",
    "WINDOW_HEIGHT = maze.row_count * CELL_SIZE\n",
    "screen = pygame.display.set_mode((WINDOW_WIDTH, WINDOW_HEIGHT))\n",
    "clock = pygame.time.Clock()\n",
    "pygame.display.set_caption(\"Q-Learning Maze\")\n",
    "\n",
    "# Color mapping for pygame maze display\n",
    "colors = {\n",
    "    \"S\": (0, 120, 255),     # Start - Bright Blue  \n",
    "    \"W\": (60, 60, 60),      # Wall - Dark Gray  \n",
    "    \"G\": (0, 200, 0),       # Goal - Vivid Green  \n",
    "    \"T\": (180, 90, 200),    # Teleport or special tile - Purple  \n",
    "    \"o\": (220, 220, 220),   # Empty - Light Gray  \n",
    "    \"+\": (255, 165, 0),     # Bonus - Orange  \n",
    "    \"-\": (200, 0, 0)        # Trap - Deep Red  \n",
    "}\n",
    "\n",
    "def display_maze():\n",
    "    \"\"\"Display maze using pygame\"\"\"\n",
    "\n",
    "    # Stop running if pygame window is closed\n",
    "    for event in pygame.event.get():\n",
    "        if event.type == pygame.QUIT:\n",
    "            pygame.quit()\n",
    "            exit()\n",
    "\n",
    "    # Draw maze\n",
    "    for idx, tile in enumerate(maze.maze):\n",
    "        row = idx // maze.column_count\n",
    "        col = idx % maze.column_count\n",
    "\n",
    "        rect = pygame.Rect(col * CELL_SIZE, row * CELL_SIZE, CELL_SIZE, CELL_SIZE)\n",
    "\n",
    "        color = colors.get(tile, (200, 200, 200))  # Default is light gray\n",
    "        pygame.draw.rect(screen, color, rect)\n",
    "\n",
    "        # Draw agent\n",
    "        if idx == agent.cur_state:\n",
    "            pygame.draw.circle(screen, (0, 0, 0), rect.center, CELL_SIZE // 3)\n",
    "\n",
    "        pygame.draw.rect(screen, (50, 50, 50), rect, 1)  # Grid lines\n",
    "\n",
    "    pygame.display.flip()\n",
    "\n",
    "    # Set FPS (higher leads to faster training)\n",
    "    clock.tick(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75514371",
   "metadata": {},
   "source": [
    "#### Terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "175ba581",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def print_maze():\n",
    "    \"\"\"Print maze in terminal\"\"\"\n",
    "\n",
    "    os.system(\"cls\")\n",
    "    cur_maze = \"\".join(maze.maze)[:maze.cur_state] + \"A\" + \"\".join(maze.maze)[maze.cur_state+1:]\n",
    "    j = maze.column_count\n",
    "    while j < len(maze.maze):\n",
    "        cur_maze = cur_maze[:j] + \"\\n\" + cur_maze[j:]\n",
    "        j += maze.column_count+1\n",
    "    print(cur_maze)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f67415",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Finally, here you can decide for how many episodes the model should be trained and with which hyperparameters it should be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4091f440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "episodes = 4000\n",
    "for i in range(episodes):\n",
    "    # Chooses an action for current state, adjusts Q-values, then performs that action\n",
    "    agent.update_q_table()\n",
    "    \n",
    "    # Display maze using pygame\n",
    "    display_maze()\n",
    "\n",
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3623bcfe",
   "metadata": {},
   "source": [
    "Let's also print out the Q-table at the end of training to see how the Q-values changed over the course of the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139a8061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maze Layout: \n",
      "ooo+ooo\n",
      "ooooooo\n",
      "SoTTToo\n",
      "ooooooG\n",
      "\n",
      "Final Q-table |||     Actions:    Left | Down | Right | Up\n",
      "Q-Value at row 1 for column 1: [ 0.  -0.3 -0.3  0. ]\n",
      "Q-Value at row 1 for column 2: [-0.3403 -0.4254 -0.3253  0.    ]\n",
      "Q-Value at row 1 for column 3: [-0.2797 -0.2851 -0.2805  0.    ]\n",
      "Q-Value at row 1 for column 4: [-0.3223 -0.3725 -0.3149  0.    ]\n",
      "Q-Value at row 1 for column 5: [-0.3201 -0.3563 -0.3224  0.    ]\n",
      "Q-Value at row 1 for column 6: [-0.2865 -0.2913 -0.2928  0.    ]\n",
      "Q-Value at row 1 for column 7: [-0.2502 -0.25    0.      0.    ]\n",
      "Q-Value at row 2 for column 1: [ 0.     -0.3371 -0.429  -0.3141]\n",
      "Q-Value at row 2 for column 2: [-0.312  -0.3182 -0.2986 -0.3071]\n",
      "Q-Value at row 2 for column 3: [-0.3105 -0.3535 -0.3144 -0.3066]\n",
      "Q-Value at row 2 for column 4: [-0.2896 -0.3095 -0.2977 -0.2877]\n",
      "Q-Value at row 2 for column 5: [-0.2863 -0.3117 -0.2779 -0.2802]\n",
      "Q-Value at row 2 for column 6: [-0.1873 -0.1835 -0.1815 -0.1845]\n",
      "Q-Value at row 2 for column 7: [-0.0718  0.0667  0.     -0.1224]\n",
      "Q-Value at row 3 for column 1: [ 0.     -0.3345 -0.4098 -0.3139]\n",
      "Q-Value at row 3 for column 2: [-0.3297 -0.3068 -0.3366 -0.3281]\n",
      "Q-Value at row 3 for column 3: [0. 0. 0. 0.]\n",
      "Q-Value at row 3 for column 4: [0. 0. 0. 0.]\n",
      "Q-Value at row 3 for column 5: [0. 0. 0. 0.]\n",
      "Q-Value at row 3 for column 6: [-0.05   -0.0328  0.0827 -0.0997]\n",
      "Q-Value at row 3 for column 7: [-0.0264  0.7743  0.     -0.0551]\n",
      "Q-Value at row 4 for column 1: [ 0.      0.     -0.2929 -0.2925]\n",
      "Q-Value at row 4 for column 2: [-0.3107  0.     -0.3054 -0.3454]\n",
      "Q-Value at row 4 for column 3: [-0.3257  0.     -0.3221 -0.3465]\n",
      "Q-Value at row 4 for column 4: [-0.2738  0.     -0.2651 -0.3366]\n",
      "Q-Value at row 4 for column 5: [-0.1109  0.     -0.0475 -0.0975]\n",
      "Q-Value at row 4 for column 6: [-0.015   0.      0.4941  0.    ]\n",
      "Q-Value at row 4 for column 7: [0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Maze Layout: \")\n",
    "print(maze_base, end=\"\\n\\n\")\n",
    "\n",
    "print(\"Final Q-table |||     Actions:    Left | Down | Right | Up\")\n",
    "q_table = agent.q_table\n",
    "for i in range(maze.row_count):\n",
    "    for j in range(i * maze.column_count, (i+1) * maze.column_count):\n",
    "        print(f\"Q-Value at row {i+1} for column {j % maze.column_count + 1}: {q_table[j]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
