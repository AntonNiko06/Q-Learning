{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c7dc51d",
   "metadata": {},
   "source": [
    "# Q-Learning\n",
    "_By Anton Nikolaychuk_\n",
    "\n",
    "In this project I will be implementing grid world and frozen lake - both basic reinforcement learning environments - and train an agent to navigate them using Q-learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64f5828",
   "metadata": {},
   "source": [
    "We will start by importing our required libraries. Numpy is used mainly for random number generation while pygame is used to display our maze and training process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07a1943",
   "metadata": {},
   "source": [
    "## TODO\n",
    "- Maze und Agent Klasse splitten\n",
    "- Informativere Docstrings\n",
    "- Annotationen passend für grid-world + frozen lake überarbeiten\n",
    "- Nicht alle Hyperparameter in Klassen init setzen, sondern als Parameter für init nehmen, die man dann beim training setzen kann\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6396ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.13.3)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from time import time\n",
    "import pygame\n",
    "\n",
    "# Prevent showing Q-values in scientific format, for better readability\n",
    "np.set_printoptions(suppress=True, precision=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d0ed09",
   "metadata": {},
   "source": [
    "## Q-Learning Implementation\n",
    "The following class implements all functions required for the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14369bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QMaze():\n",
    "    def __init__(self, maze_base):\n",
    "        \"\"\"Initialize all future required variables as well as the maze and Q-table\"\"\"\n",
    "\n",
    "        self.maze_base = maze_base\n",
    "\n",
    "        small_points = 0.1\n",
    "        goal_points = 1\n",
    "        self.step_cost = -0.04\n",
    "        self.field_mapping = {\"S\": self.step_cost, \"W\": 0, \"G\": goal_points, \"T\": -goal_points, \"o\": self.step_cost, \"+\": small_points, \"-\": -small_points}\n",
    "        self.action_mapping = {0: \"left\", 1: \"down\", 2: \"right\", 3: \"up\"}\n",
    "\n",
    "        self.maze = []\n",
    "        self.column_count = None\n",
    "        self.row_count = None\n",
    "        self.start_field = None\n",
    "        self.used_boni = []\n",
    "\n",
    "        self.q_table = None\n",
    "\n",
    "        self.epsilon = 0.15\n",
    "        self.slip_chance = 0\n",
    "\n",
    "        self.cur_state = None # int between 0 and number of fields in the maze\n",
    "\n",
    "        self.construct_maze()\n",
    "        self.construct_q_table()\n",
    "\n",
    "    def construct_maze(self):\n",
    "        \"\"\"Get the maze fields as a list of characters. Set some variables based on the maze layout\"\"\"\n",
    "\n",
    "        rows = self.maze_base.split(\"\\n\")\n",
    "        fields_str = \"\".join(rows)\n",
    "\n",
    "        for i in range(len(fields_str)):\n",
    "            self.maze.append(fields_str[i])\n",
    "\n",
    "            if fields_str[i] == \"S\": \n",
    "                self.start_field = i\n",
    "                self.cur_state = i\n",
    "\n",
    "        self.column_count = len(rows[0])\n",
    "        self.row_count = len(rows)\n",
    "\n",
    "    def construct_q_table(self, method=\"zeros\"):\n",
    "        \"\"\"Initializes the Q-table with 4 values for each field\"\"\"\n",
    "\n",
    "        if method == \"zeros\":\n",
    "            self.q_table = np.zeros((self.row_count*self.column_count, 4))\n",
    "        elif method==\"random\":\n",
    "            np.random.seed(1)\n",
    "            self.q_table = np.random.uniform(-0.3, 0.3, (self.row_count*self.column_count, 4))\n",
    "            # note: setting Q-values of the goal field to zero would be preferable, as Q-values of other fields could otherwise exceed goal reward (not critical, but notable)\n",
    "        \n",
    "\n",
    "    def update_q_table(self, learning_rate, discount_rate):\n",
    "        \"\"\"Updates the value of the previous field the agent was on using Q-learning\"\"\"\n",
    "\n",
    "        # Choose action to perform from current field\n",
    "        intended_action, slip_action = self.choose_action()\n",
    "        if slip_action == None: slip_action = intended_action\n",
    "\n",
    "        # Get the current Q-value for that action on that field\n",
    "        old_q_val = self.q_table[self.cur_state, intended_action]\n",
    "\n",
    "        # Get the new field the agent would be in after performing the chosen action\n",
    "        new_state = self.get_new_state(slip_action)\n",
    "\n",
    "        # Get the immediate reward\n",
    "        immediate_reward = self.field_mapping[self.maze[new_state]]\n",
    "        if new_state in self.used_boni: # if the reward/punishment of a bonus was already collected this episode, don't give it again\n",
    "            immediate_reward = self.step_cost\n",
    "\n",
    "        # Get the maximum Q-value of the actions that can be performed in the new field\n",
    "        max_future_q_val = max(self.q_table[new_state])\n",
    "\n",
    "        # Calculate the new Q-value for the chosen action for the current field based on TD\n",
    "        new_q_val = old_q_val + learning_rate * (immediate_reward + discount_rate * max_future_q_val - old_q_val)\n",
    "\n",
    "        # Update the Q-table with the value\n",
    "        self.q_table[self.cur_state, intended_action] = new_q_val\n",
    "\n",
    "        # Update the state to perform the action\n",
    "        self.update_state(new_state)\n",
    "\n",
    "    def action_possible(self, action):\n",
    "        \"\"\"Determine if a given action is possible from the current state\"\"\"\n",
    "\n",
    "        new_state = self.get_new_state(action)\n",
    "\n",
    "        # Prevent moving out of bounds of the maze grid\n",
    "        if new_state < 0 or new_state >= len(self.maze):\n",
    "            return False\n",
    "        # Prevent moving from end of one row to start of next or vice versa by moving right/left\n",
    "        if (not self.cur_state == 0) and (\n",
    "            (self.cur_state % self.column_count == 0 and action == 0) or # start of row\n",
    "            (self.cur_state % self.column_count == self.column_count - 1 and action == 2)): # end of row\n",
    "            return False\n",
    "        # Prevent moving onto walls\n",
    "        if self.maze[new_state] == \"W\":\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def get_possible_actions(self):\n",
    "        all_actions = np.arange(4)\n",
    "        pos_actions = []\n",
    "\n",
    "        for act in all_actions:\n",
    "            if self.action_possible(act): \n",
    "                pos_actions.append(act)\n",
    "\n",
    "        return np.array(pos_actions)\n",
    "\n",
    "    def get_new_state(self, action):\n",
    "        \"\"\"Get the state the agent will be in after performing the given action\"\"\"\n",
    "\n",
    "        if action == 0: # Move left\n",
    "            new_state = self.cur_state - 1\n",
    "        elif action == 1: # Move down\n",
    "            new_state = self.cur_state + self.column_count\n",
    "        elif action == 2: # Move right\n",
    "            new_state = self.cur_state + 1\n",
    "        elif action == 3: # Move up\n",
    "            new_state = self.cur_state - self.column_count\n",
    "        else:\n",
    "            raise KeyError(f\"Invalid action: {action}\")\n",
    "\n",
    "        return new_state\n",
    "\n",
    "    def update_state(self, new_state):\n",
    "        \"\"\"Update the current state based on the new state. If the new state is the goal field then reset the agent position to the start field.\"\"\"\n",
    "\n",
    "        self.cur_state = new_state\n",
    "\n",
    "        next_maze_field = self.maze[new_state]\n",
    "\n",
    "        if next_maze_field == \"+\" or next_maze_field == \"-\":\n",
    "            self.used_boni.append(new_state)\n",
    "\n",
    "        if next_maze_field == \"G\" or next_maze_field == \"T\":\n",
    "            self.cur_state = self.start_field\n",
    "            self.used_boni = []\n",
    "\n",
    "    def choose_action(self):\n",
    "        \"\"\"Choose action using epsilon-greedy policy\"\"\"\n",
    "\n",
    "        pos_actions = self.get_possible_actions()\n",
    "        intended_action = None\n",
    "        slip_action = None\n",
    "\n",
    "        if np.random.random() < 1-self.epsilon:\n",
    "            cur_q_vals = self.q_table[self.cur_state]\n",
    "            pos_q_vals = cur_q_vals[pos_actions]\n",
    "\n",
    "            max_q_val = np.max(pos_q_vals)\n",
    "            best_actions = np.where(cur_q_vals == max_q_val)[0]\n",
    "            best_actions = [act for act in best_actions if act in pos_actions]\n",
    "            \n",
    "            intended_action = np.random.choice(best_actions)\n",
    "        else:\n",
    "            intended_action = pos_actions[np.random.randint(0, len(pos_actions))]\n",
    "\n",
    "        if np.random.random() < self.slip_chance:\n",
    "            slip_action = pos_actions[np.random.randint(0, len(pos_actions))]\n",
    "        \n",
    "        return intended_action, slip_action\n",
    "    \n",
    "    def reset_random_seed(self):\n",
    "        \"\"\"Function to reintroduce randomness after random seed has been set once. Required if Q-table initialization is random and a random seed is set for reproducability\"\"\"\n",
    "\n",
    "        t = 1000 * time()\n",
    "        np.random.seed(int(t) % 2**32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92873ba8",
   "metadata": {},
   "source": [
    "## Setup\n",
    "In the following cell, you can adjust the layout of the maze by changing the \"maze_base\" variable. The field mappings are as follows: \"o\" - empty field, \"W\" - wall/obstacle, \"S\" - start field, \"G\" - goal field, \"T\" - trap, \"+\"/\"-\" small reward/punishment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce1c80ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chose the maze layout here (note: all rows should be the same size and all columns should be the same size)\n",
    "maze_base = \"\"\"\n",
    "ooo+ooo\n",
    "ooooooo\n",
    "SoTTToo\n",
    "ooooooG\n",
    "\"\"\"[1:-1]\n",
    "\n",
    "q = QMaze(maze_base)\n",
    "\n",
    "# Copy starting q_table for later comparison\n",
    "original_q_table = np.copy(q.q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9ef192",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c30d7d0",
   "metadata": {},
   "source": [
    "#### Pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "803e9c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize pygame window\n",
    "pygame.init()\n",
    "CELL_SIZE = 80\n",
    "WINDOW_WIDTH = q.column_count * CELL_SIZE\n",
    "WINDOW_HEIGHT = q.row_count * CELL_SIZE\n",
    "screen = pygame.display.set_mode((WINDOW_WIDTH, WINDOW_HEIGHT))\n",
    "clock = pygame.time.Clock()\n",
    "pygame.display.set_caption(\"Q-Learning Maze\")\n",
    "\n",
    "# Color mapping for pygame maze display\n",
    "colors = {\n",
    "    \"S\": (0, 120, 255),     # Start - Bright Blue  \n",
    "    \"W\": (60, 60, 60),      # Wall - Dark Gray  \n",
    "    \"G\": (0, 200, 0),       # Goal - Vivid Green  \n",
    "    \"T\": (180, 90, 200),    # Teleport or special tile - Purple  \n",
    "    \"o\": (220, 220, 220),   # Empty - Light Gray  \n",
    "    \"+\": (255, 165, 0),     # Bonus - Orange  \n",
    "    \"-\": (200, 0, 0)        # Trap - Deep Red  \n",
    "}\n",
    "\n",
    "def display_maze():\n",
    "    \"\"\"Display maze using pygame\"\"\"\n",
    "\n",
    "    # Stop running if pygame window is closed\n",
    "    for event in pygame.event.get():\n",
    "        if event.type == pygame.QUIT:\n",
    "            pygame.quit()\n",
    "            exit()\n",
    "\n",
    "    # Draw maze\n",
    "    for idx, tile in enumerate(q.maze):\n",
    "        row = idx // q.column_count\n",
    "        col = idx % q.column_count\n",
    "\n",
    "        rect = pygame.Rect(col * CELL_SIZE, row * CELL_SIZE, CELL_SIZE, CELL_SIZE)\n",
    "\n",
    "        color = colors.get(tile, (200, 200, 200))  # Default is light gray\n",
    "        pygame.draw.rect(screen, color, rect)\n",
    "\n",
    "        # Draw agent\n",
    "        if idx == q.cur_state:\n",
    "            pygame.draw.circle(screen, (0, 0, 0), rect.center, CELL_SIZE // 3)\n",
    "\n",
    "        pygame.draw.rect(screen, (50, 50, 50), rect, 1)  # Grid lines\n",
    "\n",
    "    pygame.display.flip()\n",
    "\n",
    "    # 30 FPS\n",
    "    clock.tick(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75514371",
   "metadata": {},
   "source": [
    "#### Terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175ba581",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def print_maze():\n",
    "    \"\"\"Print maze in terminal\"\"\"\n",
    "\n",
    "    os.system(\"cls\")\n",
    "    cur_maze = \"\".join(q.maze)[:q.cur_state] + \"A\" + \"\".join(q.maze)[q.cur_state+1:]\n",
    "    j = q.column_count\n",
    "    while j < len(q.maze):\n",
    "        cur_maze = cur_maze[:j] + \"\\n\" + cur_maze[j:]\n",
    "        j += q.column_count+1\n",
    "    print(cur_maze)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f67415",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Finally, here you can decide for how many episodes to train the model and with which hyperparameters it should be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4091f440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "episodes = 4000\n",
    "for i in range(episodes):\n",
    "    # Chooses an action for current state, adjusts Q-values, then performs that action\n",
    "    q.update_q_table(learning_rate=0.05, discount_rate=0.9)\n",
    "    \n",
    "    # Display maze using pygame\n",
    "    display_maze()\n",
    "\n",
    "\n",
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3623bcfe",
   "metadata": {},
   "source": [
    "Let's also print out the Q-table at the start and end of training to see how the Q-values changed over the course of the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "139a8061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maze Layout: \n",
      "ooo+ooo\n",
      "ooooooo\n",
      "SoTTToo\n",
      "ooooooG\n",
      "\n",
      "Final Q-table |||     Actions:    Left | Down | Right | Up\n",
      "Q-Value at row 1 for column 1: [ 0.    -0.038 -0.038  0.   ]\n",
      "Q-Value at row 1 for column 2: [-0.034 -0.034 -0.034  0.   ]\n",
      "Q-Value at row 1 for column 3: [-0.018 -0.02  -0.014  0.   ]\n",
      "Q-Value at row 1 for column 4: [-0.039 -0.04  -0.039  0.   ]\n",
      "Q-Value at row 1 for column 5: [-0.033 -0.034 -0.033  0.   ]\n",
      "Q-Value at row 1 for column 6: [-0.021 -0.022 -0.022  0.   ]\n",
      "Q-Value at row 1 for column 7: [-0.011 -0.004  0.     0.   ]\n",
      "Q-Value at row 2 for column 1: [ 0.    -0.04  -0.041 -0.039]\n",
      "Q-Value at row 2 for column 2: [-0.026 -0.029 -0.027 -0.026]\n",
      "Q-Value at row 2 for column 3: [-0.022 -0.05  -0.022 -0.021]\n",
      "Q-Value at row 2 for column 4: [-0.023 -0.143 -0.023 -0.023]\n",
      "Q-Value at row 2 for column 5: [-0.02 -0.05 -0.02 -0.02]\n",
      "Q-Value at row 2 for column 6: [-0.011  0.033 -0.007 -0.009]\n",
      "Q-Value at row 2 for column 7: [-0.007  0.081  0.    -0.007]\n",
      "Q-Value at row 3 for column 1: [ 0.    -0.04  -0.045 -0.04 ]\n",
      "Q-Value at row 3 for column 2: [-0.029 -0.029 -0.05  -0.029]\n",
      "Q-Value at row 3 for column 3: [0. 0. 0. 0.]\n",
      "Q-Value at row 3 for column 4: [0. 0. 0. 0.]\n",
      "Q-Value at row 3 for column 5: [0. 0. 0. 0.]\n",
      "Q-Value at row 3 for column 6: [-0.05   0.253  0.    -0.001]\n",
      "Q-Value at row 3 for column 7: [-0.001  0.431  0.     0.   ]\n",
      "Q-Value at row 4 for column 1: [ 0.    0.   -0.04 -0.04]\n",
      "Q-Value at row 4 for column 2: [-0.038  0.    -0.038 -0.04 ]\n",
      "Q-Value at row 4 for column 3: [-0.029  0.    -0.015 -0.143]\n",
      "Q-Value at row 4 for column 4: [-0.015  0.     0.095 -0.143]\n",
      "Q-Value at row 4 for column 5: [-0.004  0.     0.373 -0.185]\n",
      "Q-Value at row 4 for column 6: [0.02  0.    0.842 0.   ]\n",
      "Q-Value at row 4 for column 7: [0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Maze Layout: \")\n",
    "print(maze_base, end=\"\\n\\n\")\n",
    "\n",
    "print(\"Final Q-table |||     Actions:    Left | Down | Right | Up\")\n",
    "q_table = q.q_table\n",
    "for i in range(q.row_count):\n",
    "    for j in range(i * q.column_count, (i+1) * q.column_count):\n",
    "        print(f\"Q-Value at row {i+1} for column {j % q.column_count + 1}: {q_table[j]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
