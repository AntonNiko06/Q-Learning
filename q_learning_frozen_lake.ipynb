{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c7dc51d",
   "metadata": {},
   "source": [
    "# Q-Learning\n",
    "_By Anton Nikolaychuk_\n",
    "\n",
    "In this project I will be implementing grid world and frozen lake - both basic reinforcement learning environments - and train an agent to navigate them using Q-learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64f5828",
   "metadata": {},
   "source": [
    "We will start by importing our required libraries. Numpy is used mainly for random number generation while pygame is used to display our maze and training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6396ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.13.3)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from time import time, sleep\n",
    "import pygame\n",
    "\n",
    "# Prevent showing Q-values in scientific format, for better readability\n",
    "np.set_printoptions(suppress=True, precision=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d0ed09",
   "metadata": {},
   "source": [
    "## Q-Learning Implementation\n",
    "The following class implements all functions required for the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14369bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QMaze():\n",
    "    def __init__(self, maze_base):\n",
    "        \"\"\"Initialize all future required variables as well as the maze and Q-table\"\"\"\n",
    "\n",
    "        self.maze_base = maze_base\n",
    "\n",
    "        small_points = 0.2\n",
    "        goal_points = 1\n",
    "        self.step_cost = -0.03\n",
    "        self.field_mapping = {\"S\": self.step_cost, \"W\": 0, \"G\": goal_points, \"T\": -goal_points, \"o\": self.step_cost, \"+\": small_points, \"-\": -small_points}\n",
    "        self.action_mapping = {0: \"left\", 1: \"down\", 2: \"right\", 3: \"up\"}\n",
    "\n",
    "        self.maze = []\n",
    "        self.column_count = None\n",
    "        self.row_count = None\n",
    "        self.start_field = None\n",
    "        self.used_boni = []\n",
    "\n",
    "        self.q_table = None\n",
    "\n",
    "        self.epsilon = 0.15\n",
    "        self.slip_chance = 0.2\n",
    "\n",
    "        self.cur_state = None # int between 0 and number of fields in the maze\n",
    "\n",
    "        self.construct_maze()\n",
    "        self.construct_q_table()\n",
    "\n",
    "    def construct_maze(self):\n",
    "        \"\"\"Get the maze fields as a list of characters. Set some variables containing information about the maze layout\"\"\"\n",
    "\n",
    "        rows = self.maze_base.split(\"\\n\")\n",
    "        fields_str = \"\".join(rows)\n",
    "\n",
    "        for i in range(len(fields_str)):\n",
    "            self.maze.append(fields_str[i])\n",
    "\n",
    "            if fields_str[i] == \"S\": \n",
    "                self.start_field = i\n",
    "                self.cur_state = i\n",
    "\n",
    "        self.column_count = len(rows[0])\n",
    "        self.row_count = len(rows)\n",
    "\n",
    "    def construct_q_table(self, method=\"zeros\"):\n",
    "        \"\"\"Initializes the Q-table with 4 values for each field\"\"\"\n",
    "\n",
    "        if method == \"zeros\":\n",
    "            self.q_table = np.zeros((self.row_count*self.column_count, 4))\n",
    "        elif method==\"random\":\n",
    "            np.random.seed(1)\n",
    "            self.q_table = np.random.uniform(-0.3, 0.3, (self.row_count*self.column_count, 4))\n",
    "        \n",
    "    def update_q_table(self, learning_rate, discount_rate):\n",
    "        \"\"\"Updates the value of the previous field the agent was on using Q-learning\"\"\"\n",
    "\n",
    "        # Choose action to perform from current field\n",
    "        intended_action, slip_action = self.choose_action()\n",
    "\n",
    "        # Get the current Q-value for that action on that field\n",
    "        old_q_val = self.q_table[self.cur_state, intended_action]\n",
    "\n",
    "        # Get the new field the agent would be in after performing the chosen action\n",
    "        new_state = self.get_new_state(slip_action)\n",
    "\n",
    "        # Get the immediate reward\n",
    "        immediate_reward = self.field_mapping[self.maze[new_state]]\n",
    "        if new_state in self.used_boni: # if the reward/punishment of a bonus was already collected this episode, don't give it again\n",
    "            immediate_reward = self.step_cost\n",
    "\n",
    "        # Get the maximum Q-value of the actions that can be performed in the new field\n",
    "        max_future_q_val = max(self.q_table[new_state])\n",
    "\n",
    "        # Calculate the new Q-value for the chosen action for the current field based on TD\n",
    "        new_q_val = old_q_val + learning_rate * (immediate_reward + discount_rate * max_future_q_val - old_q_val)\n",
    "\n",
    "        # Update the Q-table with the value\n",
    "        self.q_table[self.cur_state, intended_action] = new_q_val\n",
    "\n",
    "        # Update the state to perform the action\n",
    "        self.update_state(new_state)\n",
    "\n",
    "    def action_possible(self, action):\n",
    "        \"\"\"Determine if a given action is possible at the current state\"\"\"\n",
    "\n",
    "        new_state = self.get_new_state(action)\n",
    "\n",
    "        # Prevent moving out of bounds of the maze grid\n",
    "        if new_state < 0 or new_state >= len(self.maze):\n",
    "            return False\n",
    "        # Prevent moving from end of one row to start of next or vice versa by moving right/left\n",
    "        if (not self.cur_state == 0) and (\n",
    "            (self.cur_state % self.column_count == 0 and action == 0) or # start of row\n",
    "            (self.cur_state % self.column_count == self.column_count - 1 and action == 2)): # end of row\n",
    "            return False\n",
    "        # Prevent moving onto walls\n",
    "        if self.maze[new_state] == \"W\":\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def get_possible_actions(self) -> np.ndarray:\n",
    "        \"\"\"Get a list of possible actions\"\"\"\n",
    "\n",
    "        all_actions = np.arange(4)\n",
    "        pos_actions = []\n",
    "\n",
    "        for act in all_actions:\n",
    "            if self.action_possible(act): \n",
    "                pos_actions.append(act)\n",
    "\n",
    "        return np.array(pos_actions)\n",
    "\n",
    "    def get_new_state(self, action):\n",
    "        \"\"\"Get the state the agent will be in after performing a given action\"\"\"\n",
    "\n",
    "        if action == 0: # Move left\n",
    "            new_state = self.cur_state - 1\n",
    "        elif action == 1: # Move down\n",
    "            new_state = self.cur_state + self.column_count\n",
    "        elif action == 2: # Move right\n",
    "            new_state = self.cur_state + 1\n",
    "        elif action == 3: # Move up\n",
    "            new_state = self.cur_state - self.column_count\n",
    "        else:\n",
    "            raise KeyError(f\"Invalid action: {action}\")\n",
    "\n",
    "        return new_state\n",
    "\n",
    "    def update_state(self, new_state):\n",
    "        \"\"\"Update the current state based on the new state\"\"\"\n",
    "\n",
    "        # Update state\n",
    "        self.cur_state = new_state\n",
    "\n",
    "        next_maze_field = self.maze[new_state]\n",
    "\n",
    "        # If the agent lands on a bonus field, keep track of it until the end of the episode in order to prevent using bonus multiple times\n",
    "        if next_maze_field == \"+\" or next_maze_field == \"-\":\n",
    "            self.used_boni.append(new_state)\n",
    "\n",
    "        # If the agent lands on a goal/trap field, reset his position to the start field and forget about the boni\n",
    "        if next_maze_field == \"G\" or next_maze_field == \"T\":\n",
    "            self.cur_state = self.start_field\n",
    "            self.used_boni = []\n",
    "\n",
    "    def choose_action(self):\n",
    "        \"\"\"Choose action using epsilon-greedy policy and incorporate slip chance\"\"\"\n",
    "\n",
    "        pos_actions = self.get_possible_actions()\n",
    "        intended_action = None\n",
    "        slip_action = None\n",
    "\n",
    "        if np.random.random() < 1-self.epsilon: # Choose best action based on highest Q-value\n",
    "            cur_q_vals = self.q_table[self.cur_state]\n",
    "            pos_q_vals = cur_q_vals[pos_actions]\n",
    "\n",
    "            max_q_val = np.max(pos_q_vals)\n",
    "            best_actions = np.where(cur_q_vals == max_q_val)[0]\n",
    "            best_actions = [act for act in best_actions if act in pos_actions]\n",
    "            \n",
    "            intended_action = np.random.choice(best_actions)\n",
    "        else: # Choose random action\n",
    "            intended_action = pos_actions[np.random.randint(0, len(pos_actions))]\n",
    "\n",
    "        slip_action = intended_action\n",
    "\n",
    "        # There is a random chance for the agent to \"slip\" and perform an action different from the one he intended to perform\n",
    "        if np.random.random() < self.slip_chance:\n",
    "            slip_action = pos_actions[np.random.randint(0, len(pos_actions))]\n",
    "        \n",
    "        return intended_action, slip_action\n",
    "    \n",
    "    def reset_random_seed(self):\n",
    "        \"\"\"Function to reintroduce randomness after random seed has been set once. Required if Q-table initialization is random and a random seed is set for reproducability\"\"\"\n",
    "\n",
    "        t = 1000 * time()\n",
    "        np.random.seed(int(t) % 2**32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92873ba8",
   "metadata": {},
   "source": [
    "## Setup\n",
    "In the following cell, you can adjust the layout of the maze by changing the \"maze_base\" variable. The field mappings are as follows: \n",
    "- \"o\" - empty field\n",
    "- \"W\" - wall/obstacle\n",
    "- \"S\" - start field\n",
    "- \"G\" - goal field\n",
    "- \"T\" - trap/hole in ice\n",
    "- \"+\" -  small reward\n",
    "- \"-\" - small punishment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1c80ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chose the maze layout here (all rows should be the same size and all columns should be the same size, but row and column counts do not have to match. Don't add whitespaces)\n",
    "maze_base = \"\"\"\n",
    "ooo+ooo\n",
    "ooooooo\n",
    "SoTTToo\n",
    "ooooooG\n",
    "\"\"\"[1:-1]\n",
    "\n",
    "#\n",
    "q = QMaze(maze_base)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9ef192",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c30d7d0",
   "metadata": {},
   "source": [
    "#### Pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803e9c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize pygame window\n",
    "pygame.init()\n",
    "CELL_SIZE = 80\n",
    "WINDOW_WIDTH = q.column_count * CELL_SIZE\n",
    "WINDOW_HEIGHT = q.row_count * CELL_SIZE\n",
    "screen = pygame.display.set_mode((WINDOW_WIDTH, WINDOW_HEIGHT))\n",
    "clock = pygame.time.Clock()\n",
    "pygame.display.set_caption(\"Q-Learning Maze\")\n",
    "\n",
    "# Color mapping for pygame maze display\n",
    "colors = {\n",
    "    \"S\": (0, 120, 255),     # Start - Bright Blue  \n",
    "    \"W\": (60, 60, 60),      # Wall - Dark Gray  \n",
    "    \"G\": (0, 200, 0),       # Goal - Vivid Green  \n",
    "    \"T\": (180, 90, 200),    # Teleport or special tile - Purple  \n",
    "    \"o\": (220, 220, 220),   # Empty - Light Gray  \n",
    "    \"+\": (255, 165, 0),     # Bonus - Orange  \n",
    "    \"-\": (200, 0, 0)        # Trap - Deep Red  \n",
    "}\n",
    "\n",
    "def display_maze():\n",
    "    \"\"\"Display maze using pygame\"\"\"\n",
    "\n",
    "    # Stop running if pygame window is closed\n",
    "    for event in pygame.event.get():\n",
    "        if event.type == pygame.QUIT:\n",
    "            pygame.quit()\n",
    "            exit()\n",
    "\n",
    "    # Draw maze\n",
    "    for idx, tile in enumerate(q.maze):\n",
    "        row = idx // q.column_count\n",
    "        col = idx % q.column_count\n",
    "\n",
    "        rect = pygame.Rect(col * CELL_SIZE, row * CELL_SIZE, CELL_SIZE, CELL_SIZE)\n",
    "\n",
    "        color = colors.get(tile, (200, 200, 200))  # Default is light gray\n",
    "        pygame.draw.rect(screen, color, rect)\n",
    "\n",
    "        # Draw agent\n",
    "        if idx == q.cur_state:\n",
    "            pygame.draw.circle(screen, (0, 0, 0), rect.center, CELL_SIZE // 3)\n",
    "\n",
    "        pygame.draw.rect(screen, (50, 50, 50), rect, 1)  # Grid lines\n",
    "\n",
    "    pygame.display.flip()\n",
    "\n",
    "    # 30 FPS\n",
    "    clock.tick(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75514371",
   "metadata": {},
   "source": [
    "#### Terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "175ba581",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def print_maze():\n",
    "    \"\"\"Print maze in terminal\"\"\"\n",
    "\n",
    "    os.system(\"cls\")\n",
    "    cur_maze = \"\".join(q.maze)[:q.cur_state] + \"A\" + \"\".join(q.maze)[q.cur_state+1:]\n",
    "    j = q.column_count\n",
    "    while j < len(q.maze):\n",
    "        cur_maze = cur_maze[:j] + \"\\n\" + cur_maze[j:]\n",
    "        j += q.column_count+1\n",
    "    print(cur_maze)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f67415",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Finally, here you can decide for how many episodes the model should be trained and with which hyperparameters it should be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4091f440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "episodes = 4000\n",
    "for i in range(episodes):\n",
    "    # Chooses an action for current state, adjusts Q-values, then performs that action\n",
    "    q.update_q_table(learning_rate=0.05, discount_rate=0.9)\n",
    "    \n",
    "    # Display maze using pygame\n",
    "    display_maze()\n",
    "\n",
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3623bcfe",
   "metadata": {},
   "source": [
    "Let's also print out the Q-table at the start and end of training to see how the Q-values changed over the course of the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "139a8061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maze Layout: \n",
      "ooo+ooo\n",
      "ooooooo\n",
      "SoTTToo\n",
      "ooooooG\n",
      "\n",
      "Final Q-table |||     Actions:    Left | Down | Right | Up\n",
      "Q-Value at row 1 for column 1: [ 0.    -0.03  -0.029  0.   ]\n",
      "Q-Value at row 1 for column 2: [-0.0301 -0.0355 -0.0172  0.    ]\n",
      "Q-Value at row 1 for column 3: [-0.0118 -0.0413  0.0048  0.    ]\n",
      "Q-Value at row 1 for column 4: [-0.0342 -0.0555 -0.0324  0.    ]\n",
      "Q-Value at row 1 for column 5: [-0.0316 -0.0372 -0.0289  0.    ]\n",
      "Q-Value at row 1 for column 6: [-0.0222  0.0178 -0.0219  0.    ]\n",
      "Q-Value at row 1 for column 7: [-0.0126  0.0006  0.      0.    ]\n",
      "Q-Value at row 2 for column 1: [ 0.     -0.0351 -0.0434 -0.032 ]\n",
      "Q-Value at row 2 for column 2: [-0.0356 -0.0362 -0.0428 -0.0329]\n",
      "Q-Value at row 2 for column 3: [-0.1336 -0.2262 -0.124  -0.0475]\n",
      "Q-Value at row 2 for column 4: [-0.1024 -0.0983 -0.0836 -0.0903]\n",
      "Q-Value at row 2 for column 5: [-0.0725 -0.1855 -0.0084 -0.0576]\n",
      "Q-Value at row 2 for column 6: [-0.0174  0.1861 -0.0076 -0.011 ]\n",
      "Q-Value at row 2 for column 7: [-0.0026  0.1326  0.     -0.0043]\n",
      "Q-Value at row 3 for column 1: [ 0.     -0.0346 -0.051  -0.0351]\n",
      "Q-Value at row 3 for column 2: [-0.089  -0.1125 -0.2649 -0.0579]\n",
      "Q-Value at row 3 for column 3: [0. 0. 0. 0.]\n",
      "Q-Value at row 3 for column 4: [0. 0. 0. 0.]\n",
      "Q-Value at row 3 for column 5: [0. 0. 0. 0.]\n",
      "Q-Value at row 3 for column 6: [-0.0728  0.4995  0.0159 -0.002 ]\n",
      "Q-Value at row 3 for column 7: [ 0.0105  0.5254  0.     -0.0007]\n",
      "Q-Value at row 4 for column 1: [ 0.      0.     -0.0295 -0.0296]\n",
      "Q-Value at row 4 for column 2: [-0.0336  0.     -0.0329 -0.0313]\n",
      "Q-Value at row 4 for column 3: [-0.1637  0.     -0.1    -0.2878]\n",
      "Q-Value at row 4 for column 4: [-0.1264  0.     -0.0602 -0.1855]\n",
      "Q-Value at row 4 for column 5: [-0.0391  0.     -0.0188 -0.0975]\n",
      "Q-Value at row 4 for column 6: [0.0409 0.     0.8013 0.0468]\n",
      "Q-Value at row 4 for column 7: [0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Maze Layout: \")\n",
    "print(maze_base, end=\"\\n\\n\")\n",
    "\n",
    "print(\"Final Q-table |||     Actions:    Left | Down | Right | Up\")\n",
    "q_table = q.q_table\n",
    "for i in range(q.row_count):\n",
    "    for j in range(i * q.column_count, (i+1) * q.column_count):\n",
    "        print(f\"Q-Value at row {i+1} for column {j % q.column_count + 1}: {q_table[j]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
