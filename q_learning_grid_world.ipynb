{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c7dc51d",
   "metadata": {},
   "source": [
    "# Q-Learning: Grid World\n",
    "_By Anton Nikolaychuk_\n",
    "\n",
    "In this project I will be implementing grid world, a basic reinforcement learning environment, and train an agent to navigate it using Q-learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64f5828",
   "metadata": {},
   "source": [
    "We will start by importing our required libraries. Numpy is used mainly for random number generation while pygame is used to display our maze and training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6396ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.13.3)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from time import time\n",
    "import pygame\n",
    "\n",
    "# Prevent showing Q-values in scientific format, for better readability\n",
    "np.set_printoptions(suppress=True, precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d0ed09",
   "metadata": {},
   "source": [
    "## Q-Learning Implementation\n",
    "The following class implements all functions required for the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14369bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QMaze():\n",
    "    def __init__(self, maze_base):\n",
    "        \"\"\"Initialize all future required variables as well as the maze and Q-table\"\"\"\n",
    "\n",
    "        self.maze_base = maze_base\n",
    "\n",
    "        small_points = 0.5\n",
    "        move_cost = -0.1\n",
    "        goal_points = 10\n",
    "        self.field_mapping = {\"S\": move_cost, \"W\": \"wall\", \"G\": goal_points, \"o\": move_cost, \"+\": small_points, \"-\": -small_points}\n",
    "        self.action_mapping = {0: \"left\", 1: \"down\", 2: \"right\", 3: \"up\"}\n",
    "\n",
    "        self.maze = []\n",
    "        self.column_count = None\n",
    "        self.row_count = None\n",
    "        self.start_field = None\n",
    "\n",
    "        self.q_table = None\n",
    "\n",
    "        self.epsilon = 0.15\n",
    "\n",
    "        self.cur_state = None # int between 0 and number of fields in the maze\n",
    "\n",
    "        self.construct_maze()\n",
    "        self.construct_q_table()\n",
    "\n",
    "    def construct_maze(self):\n",
    "        \"\"\"Get the maze fields as a list of characters. Set some variables based on the maze layout\"\"\"\n",
    "\n",
    "        rows = self.maze_base.split(\"\\n\")\n",
    "        fields_str = \"\".join(rows)\n",
    "\n",
    "        for i in range(len(fields_str)):\n",
    "            self.maze.append(fields_str[i])\n",
    "\n",
    "            if fields_str[i] == \"S\": \n",
    "                self.start_field = i\n",
    "                self.cur_state = i\n",
    "\n",
    "        self.column_count = len(rows[0])\n",
    "        self.row_count = len(rows)\n",
    "\n",
    "    def construct_q_table(self, method=\"zeros\"):\n",
    "        \"\"\"Initializes the Q-table with 4 values for each field\"\"\"\n",
    "\n",
    "        if method == \"zeros\":\n",
    "            self.q_table = np.zeros((self.row_count*self.column_count, 4))\n",
    "        elif method==\"random\":\n",
    "            np.random.seed(1)\n",
    "            self.q_table = np.random.uniform(-0.3, 0.3, (self.row_count*self.column_count, 4))\n",
    "            # note: setting Q-values of the goal field to zero would be preferable, as Q-values of other fields could otherwise exceed goal reward (not critical, but notable)\n",
    "        \n",
    "\n",
    "    def update_q_table(self, learning_rate, discount_rate):\n",
    "        \"\"\"Updates the value of the previous field the agent was on using Q-learning\"\"\"\n",
    "\n",
    "        # Choose action to perform from current field\n",
    "        action = self.choose_action()\n",
    "\n",
    "        # Get the current Q-value for that action on that field\n",
    "        old_q_val = self.q_table[self.cur_state, action]\n",
    "\n",
    "        # Get the new field the agent would be in after performing that the chosen action\n",
    "        new_state = self.get_new_state(action)\n",
    "\n",
    "        # Get the immediate reward\n",
    "        immediate_reward = self.field_mapping[self.maze[new_state]]\n",
    "\n",
    "        # Get the maximum Q-value of the actions that can be performed in the new field\n",
    "        max_future_q_val = max(self.q_table[new_state])\n",
    "\n",
    "        # Calculate the new Q-value for the chosen action for the current field based on TD\n",
    "        new_q_val = old_q_val + learning_rate * (immediate_reward + discount_rate * max_future_q_val - old_q_val)\n",
    "\n",
    "        # Update the Q-table with the value\n",
    "        self.q_table[self.cur_state, action] = new_q_val\n",
    "\n",
    "        # Update the state to perform the action\n",
    "        self.update_state(new_state)\n",
    "\n",
    "    def action_possible(self, action):\n",
    "        \"\"\"Determine if a given action is possible from the current state\"\"\"\n",
    "\n",
    "        new_state = self.get_new_state(action)\n",
    "\n",
    "        # Prevent moving out of bounds of the maze grid\n",
    "        if new_state < 0 or new_state >= len(self.maze):\n",
    "            return False\n",
    "        # Prevent moving from end of one row to start of next or vice versa by moving right/left\n",
    "        if (not self.cur_state == 0) and (\n",
    "            (self.cur_state % self.column_count == 0 and action == 0) or # start of row\n",
    "            (self.cur_state % self.column_count == self.column_count - 1 and action == 2)): # end of row\n",
    "            return False\n",
    "        # Prevent moving onto walls\n",
    "        if self.maze[new_state] == \"W\":\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "\n",
    "    def get_new_state(self, action):\n",
    "        \"\"\"Get the state the agent will be in after performing the given action\"\"\"\n",
    "\n",
    "        if action == 0: # Move left\n",
    "            new_state = self.cur_state - 1\n",
    "        elif action == 1: # Move down\n",
    "            new_state = self.cur_state + self.column_count\n",
    "        elif action == 2: # Move right\n",
    "            new_state = self.cur_state + 1\n",
    "        elif action == 3: # Move up\n",
    "            new_state = self.cur_state - self.column_count\n",
    "        else:\n",
    "            raise KeyError(\"Invalid action\")\n",
    "\n",
    "        return new_state\n",
    "\n",
    "    def update_state(self, new_state):\n",
    "        \"\"\"Update the current state based on the new state. If the new state is the goal field then reset the agent position to the start field.\"\"\"\n",
    "\n",
    "        self.cur_state = new_state\n",
    "\n",
    "        next_maze_field = self.maze[new_state]\n",
    "        if next_maze_field == \"G\":\n",
    "            self.cur_state = self.start_field\n",
    "\n",
    "    def choose_action(self):\n",
    "        \"\"\"Choose action using epsilon-greedy policy\"\"\"\n",
    "        self.reset_random_seed()\n",
    "\n",
    "        if np.random.random() < 1-self.epsilon:\n",
    "            current_state_q_vals = self.q_table[self.cur_state]\n",
    "            sorted_q_vals = np.sort(current_state_q_vals)\n",
    "\n",
    "            # Go through the sorted Q-values starting at the biggest and choose the action corresponding to that Q-value, if that action is possible\n",
    "            for i in range(4)[::-1]:\n",
    "                next_biggest_val = sorted_q_vals[i]\n",
    "\n",
    "                best_actions = np.where(current_state_q_vals == next_biggest_val)[0]\n",
    "\n",
    "                if (len(best_actions) > 1): # If there are multiple actions with the same highest Q-value choose a random one of those actions\n",
    "                    np.random.shuffle(best_actions)\n",
    "\n",
    "                    for act in best_actions:\n",
    "                        if self.action_possible(act): \n",
    "                            return act\n",
    "                else: # If there are not multiple best actions simply choose the one best action\n",
    "                    index_of_max_q_val = int(np.where(current_state_q_vals == next_biggest_val)[0][0])\n",
    "\n",
    "                    if self.action_possible(index_of_max_q_val):\n",
    "                        return index_of_max_q_val\n",
    "        \n",
    "        # If none of the best actions are possible or if the random number is bigger than 1-epsilon simply choose a random action\n",
    "        actions = np.arange(4)\n",
    "        np.random.shuffle(actions)\n",
    "        for act in actions:\n",
    "            if self.action_possible(act): \n",
    "                return act\n",
    "    \n",
    "    def reset_random_seed(self):\n",
    "        \"\"\"Function to reintroduce randomness after random seed has been set once. Required if Q-table initialization is random and a random seed is set for reproducability\"\"\"\n",
    "\n",
    "        t = 1000 * time()\n",
    "        np.random.seed(int(t) % 2**32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92873ba8",
   "metadata": {},
   "source": [
    "## Setup\n",
    "In the following cell, you can adjust the layout of the maze by changing the \"maze_base\" variable. The field mappings are as follows: \"o\" - empty field, \"W\" - wall/obstacle, \"S\" - start field, \"G\" - goal field, \"+\"/\"-\" small reward/punishment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce1c80ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chose the maze layout here (note: all rows should be the same size and all columns should be the same size)\n",
    "maze_base = \"\"\"\n",
    "oooooo\n",
    "oooWWo\n",
    "Soo-Wo\n",
    "ooWoGo\n",
    "\"\"\"[1:-1]\n",
    "\n",
    "q = QMaze(maze_base)\n",
    "\n",
    "# Copy starting q_table for later comparison\n",
    "original_q_table = np.copy(q.q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9ef192",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c30d7d0",
   "metadata": {},
   "source": [
    "#### Pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "803e9c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize pygame window\n",
    "pygame.init()\n",
    "CELL_SIZE = 80\n",
    "WINDOW_WIDTH = q.column_count * CELL_SIZE\n",
    "WINDOW_HEIGHT = q.row_count * CELL_SIZE\n",
    "screen = pygame.display.set_mode((WINDOW_WIDTH, WINDOW_HEIGHT))\n",
    "clock = pygame.time.Clock()\n",
    "pygame.display.set_caption(\"Q-Learning Maze\")\n",
    "\n",
    "# Color mapping for pygame maze display\n",
    "colors = {\n",
    "    \"S\": (0, 0, 255),     # Start - Blue\n",
    "    \"W\": (0, 0, 0),       # Wall - Black\n",
    "    \"G\": (0, 255, 0),     # Goal - Green\n",
    "    \"o\": (200, 200, 200), # Empty - Light Gray\n",
    "    \"+\": (255, 255, 0),   # Bonus - Yellow\n",
    "    \"-\": (255, 0, 0)      # Trap - Red\n",
    "}\n",
    "\n",
    "def display_maze():\n",
    "    \"\"\"Display maze using pygame\"\"\"\n",
    "\n",
    "    # Stop running if pygame window is closed\n",
    "    for event in pygame.event.get():\n",
    "        if event.type == pygame.QUIT:\n",
    "            pygame.quit()\n",
    "            exit()\n",
    "\n",
    "    # Draw maze\n",
    "    for idx, tile in enumerate(q.maze):\n",
    "        row = idx // q.column_count\n",
    "        col = idx % q.column_count\n",
    "\n",
    "        rect = pygame.Rect(col * CELL_SIZE, row * CELL_SIZE, CELL_SIZE, CELL_SIZE)\n",
    "\n",
    "        color = colors.get(tile, (200, 200, 200))  # Default is light gray\n",
    "        pygame.draw.rect(screen, color, rect)\n",
    "\n",
    "        # Draw agent\n",
    "        if idx == q.cur_state:\n",
    "            pygame.draw.circle(screen, (0, 0, 0), rect.center, CELL_SIZE // 3)\n",
    "\n",
    "        pygame.draw.rect(screen, (50, 50, 50), rect, 1)  # Grid lines\n",
    "\n",
    "    pygame.display.flip()\n",
    "\n",
    "    # 30 FPS\n",
    "    clock.tick(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75514371",
   "metadata": {},
   "source": [
    "#### Terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "175ba581",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def print_maze():\n",
    "    \"\"\"Print maze in terminal\"\"\"\n",
    "\n",
    "    os.system(\"cls\")\n",
    "    print(f\"EPOCH: {i}\")\n",
    "    cur_maze = \"\".join(q.maze)[:q.cur_state] + \"A\" + \"\".join(q.maze)[q.cur_state+1:]\n",
    "    j = q.column_count\n",
    "    while j < len(q.maze):\n",
    "        cur_maze = cur_maze[:j] + \"\\n\" + cur_maze[j:]\n",
    "        j += q.column_count+1\n",
    "    print(cur_maze)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f67415",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Finally, here you can decide for how many episodes to train the model and with which hyperparameters it should be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4091f440",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "display Surface quit",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31merror\u001b[39m                                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m q.update_q_table(learning_rate=\u001b[32m0.05\u001b[39m, discount_rate=\u001b[32m0.9\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Display maze using pygame\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43mdisplay_maze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 37\u001b[39m, in \u001b[36mdisplay_maze\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     34\u001b[39m rect = pygame.Rect(col * CELL_SIZE, row * CELL_SIZE, CELL_SIZE, CELL_SIZE)\n\u001b[32m     36\u001b[39m color = colors.get(tile, (\u001b[32m200\u001b[39m, \u001b[32m200\u001b[39m, \u001b[32m200\u001b[39m))  \u001b[38;5;66;03m# Default is light gray\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m \u001b[43mpygame\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdraw\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscreen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrect\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Draw agent\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m idx == q.cur_state:\n",
      "\u001b[31merror\u001b[39m: display Surface quit"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Training\n",
    "episodes = 4500\n",
    "for i in range(episodes):\n",
    "    # Chooses an action for current state, adjusts Q-values, then performs that action\n",
    "    q.update_q_table(learning_rate=0.05, discount_rate=0.9)\n",
    "\n",
    "    # Display maze using pygame\n",
    "    display_maze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3623bcfe",
   "metadata": {},
   "source": [
    "Let's also print out the Q-table at the start and end of training to see how the Q-values changed over the course of the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139a8061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Q-table: \n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "Final Q-table: \n",
      "[[ 0.    -0.1   -0.1    0.   ]\n",
      " [-0.1   -0.134 -0.1    0.   ]\n",
      " [-0.1   -0.1   -0.089  0.   ]\n",
      " [-0.098  0.     0.102  0.   ]\n",
      " [-0.082  0.     0.62   0.   ]\n",
      " [-0.057  1.827  0.     0.   ]\n",
      " [ 0.    -0.1   -0.138 -0.1  ]\n",
      " [-0.083 -0.087 -0.083 -0.083]\n",
      " [-0.113 -0.1    0.    -0.1  ]\n",
      " [ 0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.   ]\n",
      " [ 0.     3.933  0.    -0.015]\n",
      " [ 0.    -0.1   -0.152 -0.1  ]\n",
      " [-0.087 -0.086 -0.087 -0.089]\n",
      " [-0.119  0.    -0.159 -0.1  ]\n",
      " [-0.01   0.366  0.     0.   ]\n",
      " [ 0.     0.     0.     0.   ]\n",
      " [ 0.     6.697  0.     0.12 ]\n",
      " [ 0.     0.    -0.1   -0.1  ]\n",
      " [-0.1    0.     0.    -0.146]\n",
      " [ 0.     0.     0.     0.   ]\n",
      " [ 0.     0.     3.017 -0.025]\n",
      " [ 0.     0.     0.     0.   ]\n",
      " [ 9.306  0.     0.     0.445]]\n",
      "Differences between Q-tables: \n",
      "[[ 0.    -0.1   -0.1    0.   ]\n",
      " [-0.1   -0.134 -0.1    0.   ]\n",
      " [-0.1   -0.1   -0.089  0.   ]\n",
      " [-0.098  0.     0.102  0.   ]\n",
      " [-0.082  0.     0.62   0.   ]\n",
      " [-0.057  1.827  0.     0.   ]\n",
      " [ 0.    -0.1   -0.138 -0.1  ]\n",
      " [-0.083 -0.087 -0.083 -0.083]\n",
      " [-0.113 -0.1    0.    -0.1  ]\n",
      " [ 0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.   ]\n",
      " [ 0.     3.933  0.    -0.015]\n",
      " [ 0.    -0.1   -0.152 -0.1  ]\n",
      " [-0.087 -0.086 -0.087 -0.089]\n",
      " [-0.119  0.    -0.159 -0.1  ]\n",
      " [-0.01   0.366  0.     0.   ]\n",
      " [ 0.     0.     0.     0.   ]\n",
      " [ 0.     6.697  0.     0.12 ]\n",
      " [ 0.     0.    -0.1   -0.1  ]\n",
      " [-0.1    0.     0.    -0.146]\n",
      " [ 0.     0.     0.     0.   ]\n",
      " [ 0.     0.     3.017 -0.025]\n",
      " [ 0.     0.     0.     0.   ]\n",
      " [ 9.306  0.     0.     0.445]]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Print out Q-table before and after training for comparison\n",
    "print(\"Original Q-table: \")\n",
    "print(original_q_table)\n",
    "print(\"Final Q-table: \")\n",
    "print(q.q_table)\n",
    "print(\"Differences between Q-tables: \")\n",
    "print(q.q_table - original_q_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
