{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c7dc51d",
   "metadata": {},
   "source": [
    "# Q-Learning\n",
    "\n",
    "In this project I will be implementing the Gridworld and Frozen Lake reinforcement learning environments and train an agent to navigate them using Q-learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64f5828",
   "metadata": {},
   "source": [
    "We will start by importing our required libraries. Numpy is used mainly for random number generation while pygame is used to display our maze and training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6396ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.11.4)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pygame\n",
    "\n",
    "# Prevent showing Q-values in scientific format, for better readability\n",
    "np.set_printoptions(suppress=True, precision=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d0ed09",
   "metadata": {},
   "source": [
    "## Q-Learning Implementation\n",
    "In the following cell, we implement two seperate classes, one for the maze and one for the agent. The maze class stores all information relevant to the layout of the maze, such as column/row count, while the agent class implements the required functions for learning to navigate the maze, such as the Q-function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14369bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QMaze():\n",
    "    def __init__(self, maze_base, goal_points, small_points, step_cost):\n",
    "        \"\"\"Initialize the maze using the given maze layout\"\"\"\n",
    "\n",
    "        self.maze_base = maze_base\n",
    "\n",
    "        self.step_cost = step_cost\n",
    "        self.field_mapping = {\"S\": self.step_cost, \"W\": 0, \"G\": goal_points, \"T\": -goal_points, \"o\": self.step_cost, \"+\": small_points, \"-\": -small_points}\n",
    "        self.action_mapping = {0: \"left\", 1: \"down\", 2: \"right\", 3: \"up\"}\n",
    "\n",
    "        self.maze = []\n",
    "        self.column_count = None\n",
    "        self.row_count = None\n",
    "        self.start_field = None\n",
    "\n",
    "        self.construct_maze()\n",
    "\n",
    "    def construct_maze(self):\n",
    "        \"\"\"Get the maze fields as a list of characters and store information about the maze layout\"\"\"\n",
    "\n",
    "        rows = self.maze_base.split(\"\\n\")\n",
    "        fields_str = \"\".join(rows)\n",
    "\n",
    "        for i in range(len(fields_str)):\n",
    "            self.maze.append(fields_str[i])\n",
    "\n",
    "            if fields_str[i] == \"S\": \n",
    "                self.start_field = i\n",
    "\n",
    "        self.column_count = len(rows[0])\n",
    "        self.row_count = len(rows)\n",
    "\n",
    "\n",
    "class QAgent():\n",
    "    def __init__(self, maze, epsilon, min_epsilon, slip_chance, learning_rate, discount_rate, episodes):\n",
    "        \"\"\"Initialize an instance of the agent with the given hyperparameters\"\"\"\n",
    "\n",
    "        self.maze = maze\n",
    "        self.used_boni = []\n",
    "\n",
    "        self.epsilon = epsilon\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.slip_chance = slip_chance\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_rate = discount_rate\n",
    "\n",
    "        self.episodes = episodes\n",
    "        self.cur_episode = 0\n",
    "\n",
    "        self.cur_state = self.maze.start_field\n",
    "\n",
    "        self.q_table = None\n",
    "        self.construct_q_table(method=\"zeros\")\n",
    "    \n",
    "    def construct_q_table(self, method=\"zeros\"):\n",
    "        \"\"\"Initializes the Q-table with 4 values for each field corresponding to left, down, right, up\"\"\"\n",
    "\n",
    "        if method == \"zeros\":\n",
    "            self.q_table = np.zeros((self.maze.row_count*self.maze.column_count, 4))\n",
    "        elif method==\"random\":\n",
    "            rng = np.random.default_rng(1) # rng with seed 1 instead of random.seed to not overwrite global random seed\n",
    "            self.q_table = rng.uniform(-0.3, 0.3, (self.maze.row_count*self.maze.column_count, 4))\n",
    "        \n",
    "    def update_q_table(self):\n",
    "        \"\"\"Updates the Q-value of the agent for a chosen action on the current field using Q-function\"\"\"\n",
    "\n",
    "        # Choose action to perform from current field\n",
    "        intended_action, slip_action = self.choose_action()\n",
    "\n",
    "        # Get the current Q-value for that action on that field\n",
    "        old_q_val = self.q_table[self.cur_state, intended_action]\n",
    "\n",
    "        # Get the new field the agent would land on after performing the chosen action\n",
    "        new_state = self.get_new_state(slip_action)\n",
    "\n",
    "        # Get the immediate reward of landing on that field\n",
    "        immediate_reward = self.maze.field_mapping[self.maze.maze[new_state]]\n",
    "        if new_state in self.used_boni: # don't give bonus rewards/punishments more than once per episode\n",
    "            immediate_reward = self.maze.step_cost\n",
    "\n",
    "        # Get the maximum Q-value of the actions that can be performed in the new field\n",
    "        max_future_q_val = max(self.q_table[new_state])\n",
    "\n",
    "        # Calculate the new Q-value for the chosen action for the current field using the Q-function\n",
    "        new_q_val = old_q_val + self.learning_rate * (immediate_reward + self.discount_rate * max_future_q_val - old_q_val)\n",
    "\n",
    "        # Update the Q-table with the value\n",
    "        self.q_table[self.cur_state, intended_action] = new_q_val\n",
    "\n",
    "        # Update the state to perform the action\n",
    "        self.update_state(new_state)\n",
    "\n",
    "    def action_possible(self, action):\n",
    "        \"\"\"Determine if a given action is possible from current state\"\"\"\n",
    "\n",
    "        new_state = self.get_new_state(action)\n",
    "\n",
    "        # Prevent moving out of bounds of the maze grid\n",
    "        if new_state < 0 or new_state >= len(self.maze.maze):\n",
    "            return False\n",
    "        # Prevent moving from end of one row to start of next or vice versa by moving right/left\n",
    "        if (not self.cur_state == 0) and (\n",
    "            (self.cur_state % self.maze.column_count == 0 and action == 0) or # start of row\n",
    "            (self.cur_state % self.maze.column_count == self.maze.column_count - 1 and action == 2)): # end of row\n",
    "            return False\n",
    "        # Prevent moving onto walls\n",
    "        if self.maze.maze[new_state] == \"W\":\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def get_possible_actions(self) -> np.ndarray:\n",
    "        \"\"\"Get a list of possible actions from current state\"\"\"\n",
    "\n",
    "        all_actions = np.arange(4)\n",
    "        pos_actions = []\n",
    "\n",
    "        for act in all_actions:\n",
    "            if self.action_possible(act): \n",
    "                pos_actions.append(act)\n",
    "\n",
    "        return np.array(pos_actions)\n",
    "\n",
    "    def get_new_state(self, action):\n",
    "        \"\"\"Get the state (i.e. field) the agent will be in after performing a given action\"\"\"\n",
    "\n",
    "        if action == 0: # Move left\n",
    "            new_state = self.cur_state - 1\n",
    "        elif action == 1: # Move down\n",
    "            new_state = self.cur_state + self.maze.column_count\n",
    "        elif action == 2: # Move right\n",
    "            new_state = self.cur_state + 1\n",
    "        elif action == 3: # Move up\n",
    "            new_state = self.cur_state - self.maze.column_count\n",
    "        else:\n",
    "            raise KeyError(f\"Invalid action: {action}\")\n",
    "\n",
    "        return new_state\n",
    "\n",
    "    def update_state(self, new_state):\n",
    "        \"\"\"Update the current state (i.e. the agents position) based on the new state\"\"\"\n",
    "\n",
    "        # Update state\n",
    "        self.cur_state = new_state\n",
    "\n",
    "        next_maze_field = self.maze.maze[new_state]\n",
    "\n",
    "        # If the agent lands on a bonus field, keep track of it until the end of the episode in order to prevent using bonus multiple times\n",
    "        if next_maze_field == \"+\" or next_maze_field == \"-\":\n",
    "            self.used_boni.append(new_state)\n",
    "\n",
    "        # If the agent lands on a goal/trap field, reset its position to the start field and forget about the boni it collected\n",
    "        if next_maze_field == \"G\" or next_maze_field == \"T\":\n",
    "            self.cur_state = self.maze.start_field\n",
    "            self.cur_episode += 1\n",
    "            self.used_boni = []\n",
    "\n",
    "    def choose_action(self):\n",
    "        \"\"\"Choose action using epsilon-greedy policy and incorporate slip chance\"\"\"\n",
    "\n",
    "        pos_actions = self.get_possible_actions()\n",
    "        intended_action = None\n",
    "        slip_action = None\n",
    "\n",
    "        # Decay of exploration rate over course of training to shift from exploration to exploitation behaviour\n",
    "        epsilon = max((1 - self.cur_episode / self.episodes) * self.epsilon, self.min_epsilon)\n",
    "\n",
    "        if np.random.random() < 1-epsilon: \n",
    "            # Choose best action based on highest Q-value\n",
    "\n",
    "            cur_q_vals = self.q_table[self.cur_state] # get the Q-values for all actions of the current field\n",
    "            pos_q_vals = cur_q_vals[pos_actions] # filter out the actions that can't be performed\n",
    "\n",
    "            # Get list of actions that have the highest Q-value (as there might be multiple)\n",
    "            max_q_val = np.max(pos_q_vals)\n",
    "            best_actions = np.where(cur_q_vals == max_q_val)[0]\n",
    "            best_actions = [act for act in best_actions if act in pos_actions]\n",
    "            \n",
    "            # Randomly pick one of the best actions or the one best action (if there is just one)\n",
    "            intended_action = np.random.choice(best_actions)\n",
    "        else: \n",
    "            # Choose random action\n",
    "            intended_action = pos_actions[np.random.randint(0, len(pos_actions))]\n",
    "\n",
    "        slip_action = intended_action\n",
    "\n",
    "        # There is a random chance for the agent to \"slip\" and perform an action different from the one he intended to perform\n",
    "        # -> If slip_chance is set to zero during initialization of the agent (see cells below), we get the Gridworld\n",
    "        #    environment, otherwise the Frozen Lake environment\n",
    "        if np.random.random() < self.slip_chance:\n",
    "            slip_action = pos_actions[np.random.randint(0, len(pos_actions))]\n",
    "        \n",
    "        return intended_action, slip_action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92873ba8",
   "metadata": {},
   "source": [
    "## Setup\n",
    "In the following cell, you can adjust the layout of the maze by changing the \"maze_base\" variable. The field mappings are as follows: \n",
    "- \"o\" - empty field\n",
    "- \"W\" - wall/obstacle\n",
    "- \"S\" - start field\n",
    "- \"G\" - goal field\n",
    "- \"T\" - trap/hole in ice\n",
    "- \"+\" -  small reward\n",
    "- \"-\" - small punishment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce1c80ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chose the maze layout here (all rows should be the same size and all columns should be the same size, but row and column counts do not have to match. Don't add whitespaces)\n",
    "maze_base = \"\"\"\n",
    "oo+ooo\n",
    "oooooo\n",
    "SoTToo\n",
    "WooooG\n",
    "\"\"\"[1:-1]\n",
    "\n",
    "# Here you can assign the rewards/punishments for landing on certain fields\n",
    "# +-goal_points: G/T  |  +-small_points: +/-  |  step_cost: o (note that step cost should be negative)\n",
    "maze = QMaze(maze_base, goal_points=1, small_points=0.2, step_cost=-0.3)\n",
    "\n",
    "# Here you can change the hyperparameters for the training of the agent\n",
    "# epsilon: Probability of the agent to perform a random action instead of the best one he knows\n",
    "# min_epsilon: Minimum value epsilon should be able to take (important because epsilon decay is implemented)\n",
    "# slip_chance: Probability of the agent to \"slip\" and perform a random action instead of the one he intended to perform\n",
    "#              -> If set to 0, then the environment becomes the Gridworld environment\n",
    "#              -> If set to a value between 0 and 1, then the environment becomes the frozen lake environment\n",
    "# learning_rate: Governs how much the agent should learn with each update\n",
    "# discount_rate: Governs how important future rewards are during learning\n",
    "# episodes: How many full traversals the agent should be trained for\n",
    "agent = QAgent(maze, epsilon=0.15, min_epsilon=0.05, slip_chance=0.2, learning_rate=0.05, \n",
    "               discount_rate=0.9, episodes=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9ef192",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "\n",
    "Here we implement two ways of visualizing the agent traversing the maze, one using pygame and one using just the terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c30d7d0",
   "metadata": {},
   "source": [
    "#### Pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "803e9c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize pygame window\n",
    "pygame.init()\n",
    "CELL_SIZE = 80\n",
    "WINDOW_WIDTH = maze.column_count * CELL_SIZE\n",
    "WINDOW_HEIGHT = maze.row_count * CELL_SIZE\n",
    "screen = pygame.display.set_mode((WINDOW_WIDTH, WINDOW_HEIGHT))\n",
    "clock = pygame.time.Clock()\n",
    "pygame.display.set_caption(\"Q-Learning Maze\")\n",
    "\n",
    "# Color mapping for pygame maze display\n",
    "colors = {\n",
    "    \"S\": (0, 120, 255), # Start\n",
    "    \"W\": (60, 60, 60), # Wall\n",
    "    \"G\": (0, 200, 0), # Goal\n",
    "    \"T\": (200, 0, 0), # Trap\n",
    "    \"o\": (220, 220, 220), # Empty\n",
    "    \"+\": (195, 217, 50), # + Bonus \n",
    "    \"-\": (217, 100, 50) # - Bonus\n",
    "}\n",
    "\n",
    "def display_maze(fps):\n",
    "    \"\"\"Display maze using pygame\"\"\"\n",
    "\n",
    "    # Stop running if pygame window is closed or any key is pressed\n",
    "    for event in pygame.event.get():\n",
    "        if event.type == pygame.QUIT or event.type == pygame.KEYDOWN:\n",
    "            pygame.quit()\n",
    "            exit()\n",
    "\n",
    "    # Draw maze\n",
    "    for idx, tile in enumerate(maze.maze):\n",
    "        row = idx // maze.column_count\n",
    "        col = idx % maze.column_count\n",
    "\n",
    "        rect = pygame.Rect(col * CELL_SIZE, row * CELL_SIZE, CELL_SIZE, CELL_SIZE)\n",
    "\n",
    "        color = colors.get(tile, (200, 200, 200))  # Default is light gray\n",
    "        pygame.draw.rect(screen, color, rect)\n",
    "\n",
    "        # Draw agent\n",
    "        if idx == agent.cur_state:\n",
    "            pygame.draw.circle(screen, (0, 0, 0), rect.center, CELL_SIZE // 3)\n",
    "\n",
    "        pygame.draw.rect(screen, (50, 50, 50), rect, 1)  # Grid lines\n",
    "\n",
    "    pygame.display.flip()\n",
    "\n",
    "    # Set FPS (higher leads to faster training)\n",
    "    clock.tick(fps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75514371",
   "metadata": {},
   "source": [
    "#### Terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "175ba581",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import system\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def print_maze(method):\n",
    "    \"\"\"Print maze in terminal\"\"\"\n",
    "\n",
    "    if method == \"terminal\":\n",
    "        system(\"cls\")\n",
    "    elif method == \"notebook\":\n",
    "        clear_output(wait=True)\n",
    "\n",
    "    cur_maze = \"\".join(maze.maze)[:agent.cur_state] + \"A\" + \"\".join(maze.maze)[agent.cur_state+1:]\n",
    "    j = maze.column_count\n",
    "    while j < len(maze.maze):\n",
    "        cur_maze = cur_maze[:j] + \"\\n\" + cur_maze[j:]\n",
    "        j += maze.column_count+1\n",
    "    print(cur_maze)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f67415",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Finally, the training loop is started. The agent will continue updating it's Q-values until it has reached a terminal state (a trap or the goal) as often as the number of episodes specified during setup.\n",
    "\n",
    "You can stop training at any time by pressing any key on the keyboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4091f440",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import modules\n",
    "from time import sleep\n",
    "\n",
    "# Training - stopped by pressing any key\n",
    "while agent.cur_episode < agent.episodes:\n",
    "    # Chooses an action for current state, adjusts Q-values, then performs that action\n",
    "    agent.update_q_table()\n",
    "\n",
    "    if not \"google.colab\" in modules:\n",
    "        # Display maze using pygame (only works locally, not in Google Colab)\n",
    "        display_maze(fps=60)\n",
    "    else:\n",
    "        # Display maze using terminal/jupyter output\n",
    "        print_maze(method=\"notebook\") # if running in terminal, change to \"terminal\"\n",
    "\n",
    "        # Change how quickly the agent should traverse the maze\n",
    "        slow, medium, fast = 0.8, 0.2, 0.05\n",
    "        sleep(medium)\n",
    "\n",
    "if not \"google.colab\" in modules:\n",
    "    pygame.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3623bcfe",
   "metadata": {},
   "source": [
    "Let's also print out the Q-table at the end of training to see how the Q-values changed over the course of the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "139a8061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maze Layout: \n",
      "oo+ooo\n",
      "oooooo\n",
      "SoTToo\n",
      "WooooG\n",
      "\n",
      "Final Q-table |||     Actions:    Left | Down | Right | Up\n",
      "Q-Value at row 1 for column 1: [ 0.  -0.3 -0.3  0. ]\n",
      "Q-Value at row 1 for column 2: [-0.3424 -0.3947 -0.2818  0.    ]\n",
      "Q-Value at row 1 for column 3: [-0.3497 -0.3763 -0.3417  0.    ]\n",
      "Q-Value at row 1 for column 4: [-0.308  -0.3091 -0.3001  0.    ]\n",
      "Q-Value at row 1 for column 5: [-0.2832 -0.2794 -0.2774  0.    ]\n",
      "Q-Value at row 1 for column 6: [-0.1912 -0.1842  0.      0.    ]\n",
      "Q-Value at row 2 for column 1: [ 0.     -0.3372 -0.3597 -0.3202]\n",
      "Q-Value at row 2 for column 2: [-0.3046 -0.3084 -0.3195 -0.308 ]\n",
      "Q-Value at row 2 for column 3: [-0.3206 -0.3197 -0.3283 -0.343 ]\n",
      "Q-Value at row 2 for column 4: [-0.2591 -0.2649 -0.245  -0.2452]\n",
      "Q-Value at row 2 for column 5: [-0.1742 -0.162  -0.1586 -0.1748]\n",
      "Q-Value at row 2 for column 6: [-0.0602  0.173   0.     -0.0455]\n",
      "Q-Value at row 3 for column 1: [ 0.      0.     -0.4177 -0.3092]\n",
      "Q-Value at row 3 for column 2: [-0.2956 -0.3284 -0.3081 -0.3145]\n",
      "Q-Value at row 3 for column 3: [0. 0. 0. 0.]\n",
      "Q-Value at row 3 for column 4: [0. 0. 0. 0.]\n",
      "Q-Value at row 3 for column 5: [-0.0661 -0.0103 -0.0409 -0.0504]\n",
      "Q-Value at row 3 for column 6: [0.     0.7933 0.     0.    ]\n",
      "Q-Value at row 4 for column 1: [0. 0. 0. 0.]\n",
      "Q-Value at row 4 for column 2: [ 0.      0.     -0.2648 -0.2717]\n",
      "Q-Value at row 4 for column 3: [-0.2458  0.     -0.2328 -0.2701]\n",
      "Q-Value at row 4 for column 4: [-0.0653  0.     -0.0521 -0.0975]\n",
      "Q-Value at row 4 for column 5: [0.     0.     0.5269 0.0311]\n",
      "Q-Value at row 4 for column 6: [0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Maze Layout: \")\n",
    "print(maze_base, end=\"\\n\\n\")\n",
    "\n",
    "print(\"Final Q-table |||     Actions:    Left | Down | Right | Up\")\n",
    "q_table = agent.q_table\n",
    "for i in range(maze.row_count):\n",
    "    for j in range(i * maze.column_count, (i+1) * maze.column_count):\n",
    "        print(f\"Q-Value at row {i+1} for column {j % maze.column_count + 1}: {q_table[j]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
